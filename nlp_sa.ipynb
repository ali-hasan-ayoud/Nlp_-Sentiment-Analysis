{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import nltk \n",
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Embedding,Flatten,Dense,SpatialDropout1D,LSTM\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4_5796360327922715229.pdf', 'data.txt', 'first', 'Git.pdf', 'Managing_environments_â€”_conda_4_10_1_post10+f47dc5e6f_documentation.pdf', 'minst code', 'NLP - Twitter Sentiment Analysis Project _ Kaggle.pdf', 'PEP 8 -- Style Guide for Python Code _ Python.org.pdf', 'project four', 'sesoin four']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.listdir(r\"C:\\Users\\Ali\\Desktop\\data sentiment\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"C:\\Users\\Ali\\Desktop\\data sentiment\\project four\\New folder\\c.csv\") \n",
    "test_df = pd.read_csv(r\"C:\\Users\\Ali\\Desktop\\data sentiment\\project four\\New folder\\c.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>positive</th>\n",
       "      <th>126415614616154112</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126404574230740992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126402758403305474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126397179614068736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126395626979196928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126394830791254016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126379685453119488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126377656416612353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126373779483004928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126366353757179904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126366123368267776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   apple  positive  126415614616154112\n",
       "0  apple  positive  126404574230740992\n",
       "1  apple  positive  126402758403305474\n",
       "2  apple  positive  126397179614068736\n",
       "3  apple  positive  126395626979196928\n",
       "4  apple  positive  126394830791254016\n",
       "5  apple  positive  126379685453119488\n",
       "6  apple  positive  126377656416612353\n",
       "7  apple  positive  126373779483004928\n",
       "8  apple  positive  126366353757179904\n",
       "9  apple  positive  126366123368267776"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>positive</th>\n",
       "      <th>126415614616154112</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126404574230740992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126402758403305474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126397179614068736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126395626979196928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126394830791254016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126379685453119488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126377656416612353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126373779483004928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126366353757179904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126366123368267776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   apple  positive  126415614616154112\n",
       "0  apple  positive  126404574230740992\n",
       "1  apple  positive  126402758403305474\n",
       "2  apple  positive  126397179614068736\n",
       "3  apple  positive  126395626979196928\n",
       "4  apple  positive  126394830791254016\n",
       "5  apple  positive  126379685453119488\n",
       "6  apple  positive  126377656416612353\n",
       "7  apple  positive  126373779483004928\n",
       "8  apple  positive  126366353757179904\n",
       "9  apple  positive  126366123368267776"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"clean the data\"\n",
    "\"A converting html entites\"\n",
    "\n",
    "from html.parser import HTMLParser \n",
    "html_parser = HTMLParser() \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TweetText'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TweetText'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f8f11e4095f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_tweet'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TweetText\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhtml_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TweetText'"
     ]
    }
   ],
   "source": [
    "test_df['clean_tweet'] = test_df[\"TweetText\"].apply(lambda x: html_parser.unescape(x)) \n",
    "test_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"B Removing @user from all the tweets\"\n",
    "def remove_pattern(input_txt, pattern):    \n",
    "    r = re.findall(pattern, input_txt)    \n",
    "    for i in r:        \n",
    "        input_txt = re.sub(i, '', input_txt)    \n",
    "    return input_txt \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"clean_tweet\"]=np.vectorize(remove_pattern)(test_df[\"clean_tweet\"],\"@[\\w]*\")\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C Changing all the tweets into lowercase\"\n",
    "test_df[\"clean_tweet\"]=test_df[\"clean_tweet\"].apply(lambda x:x.lower())\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"D Apostrophe Lookup\"\n",
    "apostrophe_dict = { \"ain't\": \"am not / are not\",\n",
    "                   \"aren't\": \"are not / am not\", \n",
    "                   \"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \n",
    "                   \"'cause\": \"because\", \n",
    "                   \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \n",
    "                   \"couldn't've\": \"could not have\", \n",
    "                   \"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \n",
    "                   \"don't\": \"do not\", \n",
    "                   \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \n",
    "                   \"hasn't\": \"has not\", \n",
    "                   \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he had / he would\", \n",
    "                   \"he'd've\": \"he would have\", \n",
    "                   \"he'll\": \"he shall / he will\", \n",
    "                   \"he'll've\": \"he shall have / he will have\", \n",
    "                   \"he's\": \"he has / he is\", \n",
    "                   \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \n",
    "                   \"how'll\": \"how will\", \n",
    "                   \"how's\": \"how has / how is\", \n",
    "                   \"i'd\": \"I had / I would\", \n",
    "                   \"i'd've\": \"I would have\", \n",
    "                   \"i'll\": \"I shall / I will\", \n",
    "                   \"i'll've\": \"I shall have / I will have\", \n",
    "                   \"i'm\": \"I am\", \n",
    "                   \"i've\": \"I have\", \n",
    "                   \"isn't\": \"is not\", \n",
    "                   \"it'd\": \"it had / it would\", \n",
    "                   \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it shall / it will\", \n",
    "                   \"it'll've\": \"it shall have / it will have\", \n",
    "                   \"it's\": \"it has / it is\", \n",
    "                   \"let's\": \"let us\", \n",
    "                   \"ma'am\": \"madam\", \n",
    "                   \"mayn't\": \"may not\",\n",
    "                   \"might've\": \"might have\", \n",
    "                   \"mightn't\": \"might not\", \n",
    "                   \"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \n",
    "                   \"mustn't\": \"must not\", \n",
    "                   \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \n",
    "                   \"needn't've\": \"need not have\", \n",
    "                   \"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \n",
    "                   \"oughtn't've\": \"ought not have\", \n",
    "                   \"shan't\": \"shall not\", \n",
    "                   \"sha'n't\": \"shall not\", \n",
    "                   \"shan't've\": \"shall not have\", \n",
    "                   \"she'd\": \"she had / she would\", \n",
    "                   \"she'd've\": \"she would have\", \n",
    "                   \"she'll\": \"she shall / she will\", \n",
    "                   \"she'll've\": \"she shall have / she will have\", \n",
    "                   \"she's\": \"she has / she is\", \n",
    "                   \"should've\": \"should have\", \n",
    "                   \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \n",
    "                   \"so've\": \"so have\", \n",
    "                   \"so's\": \"so as / so is\",\n",
    "                   \"that'd\": \"that would / that had\", \n",
    "                   \"that'd've\": \"that would have\", \n",
    "                   \"that's\": \"that has / that is\", \n",
    "                   \"there'd\": \"there had / there would\", \n",
    "                   \"there'd've\": \"there would have\",\n",
    "                   \"there's\": \"there has / there is\",\n",
    "                   \"they'd\": \"they had / they would\", \n",
    "                   \"they'd've\": \"they would have\", \n",
    "                   \"they'll\": \"they shall / they will\", \n",
    "                   \"they'll've\": \"they shall have / they will have\", \n",
    "                   \"they're\": \"they are\", \n",
    "                   \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \n",
    "                   \"wasn't\": \"was not\", \n",
    "                   \"we'd\": \"we had / we would\", \n",
    "                   \"we'd've\": \"we would have\",\n",
    "                   \"we'll\": \"we will\", \n",
    "                   \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \n",
    "                   \"we've\": \"we have\", \n",
    "                   \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what shall / what will\", \n",
    "                   \"what'll've\": \"what shall have / what will have\",\n",
    "                   \"what're\": \"what are\", \n",
    "                   \"what's\": \"what has / what is\", \n",
    "                   \"what've\": \"what have\", \n",
    "                   \"when's\": \"when has / when is\", \n",
    "                   \"when've\": \"when have\", \n",
    "                   \"where'd\": \"where did\", \n",
    "                   \"where's\": \"where has / where is\", \n",
    "                   \"where've\": \"where have\", \n",
    "                   \"who'll\": \"who shall / who will\",\n",
    "                   \"who'll've\": \"who shall have / who will have\", \n",
    "                   \"who's\": \"who has / who is\", \n",
    "                   \"who've\": \"who have\", \n",
    "                   \"why's\": \"why has / why is\", \n",
    "                   \"why've\": \"why have\", \n",
    "                   \"will've\": \"will have\", \n",
    "                   \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \n",
    "                   \"would've\": \"would have\", \n",
    "                   \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \n",
    "                   \"y'all\": \"you all\", \n",
    "                   \"y'all'd\": \"you all would\", \n",
    "                   \"y'all'd've\": \"you all would have\", \n",
    "                   \"y'all're\": \"you all are\", \n",
    "                   \"y'all've\": \"you all have\", \n",
    "                   \"you'd\": \"you had / you would\", \n",
    "                   \"you'd've\": \"you would have\", \n",
    "                   \"you'll\": \"you shall / you will\", \n",
    "                   \"you'll've\": \"you shall have / you will have\", \n",
    "                   \"you're\": \"you are\", \n",
    "                   \"you've\": \"you have\" \n",
    "                  } \n",
    "apostrophe_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_dict(text, dictionary):    \n",
    "    for word in text.split():        \n",
    "        if word.lower() in dictionary:            \n",
    "            if word.lower() in text.split():               \n",
    "                text = text.replace(word, dictionary[word.lower()])    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_tweet'] = test_df['clean_tweet'].apply(lambda x: lookup_dict(x,apostrophe_dict)) \n",
    "test_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_word_dict = {\n",
    "    \"121\": \"one to one\", \n",
    "    \"a/s/l\": \"age, sex, location\", \n",
    "    \"adn\": \"any day now\", \n",
    "    \"afaik\": \"as far as I know\", \n",
    "    \"afk\": \"away from keyboard\", \n",
    "    \"aight\": \"alright\", \n",
    "    \"alol\": \"actually laughing out loud\", \n",
    "    \"b4\": \"before\", \n",
    "    \"b4n\": \"bye for now\", \n",
    "    \"bak\": \"back at the keyboard\", \n",
    "    \"bf\": \"boyfriend\", \n",
    "    \"bff\": \"best friends forever\", \n",
    "    \"bfn\": \"bye for now\", \n",
    "    \"bg\": \"big grin\", \n",
    "    \"bta\": \"but then again\", \n",
    "    \"btw\": \"by the way\", \n",
    "    \"cid\": \"crying in disgrace\", \n",
    "    \"cnp\": \"continued in my next post\", \n",
    "    \"cp\": \"chat post\", \n",
    "    \"cu\": \"see you\", \n",
    "    \"cul\": \"see you later\", \n",
    "    \"cul8r\": \"see you later\", \n",
    "    \"cya\": \"bye\", \n",
    "    \"cyo\": \"see you online\", \n",
    "    \"dbau\": \"doing business as usual\", \n",
    "    \"fud\": \"fear, uncertainty, and doubt\", \n",
    "    \"fwiw\": \"for what it's worth\", \n",
    "    \"fyi\": \"for your information\", \n",
    "    \"g\": \"grin\", \n",
    "    \"g2g\": \"got to go\", \n",
    "    \"ga\": \"go ahead\", \n",
    "    \"gal\": \"get a life\", \n",
    "    \"gf\": \"girlfriend\", \n",
    "    \"gfn\": \"gone for now\", \n",
    "    \"gmbo\": \"giggling my butt off\", \n",
    "    \"gmta\": \"great minds think alike\", \n",
    "    \"h8\": \"hate\", \"hagn\": \"have a good night\", \n",
    "    \"hdop\": \"help delete online predators\", \n",
    "    \"hhis\": \"hanging head in shame\",\n",
    "    \"iac\": \"in any case\", \n",
    "    \"ianal\": \"I am not a lawyer\", \n",
    "    \"ic\": \"I see\", \n",
    "    \"idk\": \"I don't know\", \n",
    "    \"imao\": \"in my arrogant opinion\", \n",
    "    \"imnsho\": \"in my not so humble opinion\",\n",
    "    \"imo\": \"in my opinion\", \n",
    "    \"iow\": \"in other words\", \n",
    "    \"ipn\": \"Iâ€™m posting naked\",\n",
    "    \"irl\": \"in real life\", \n",
    "    \"jk\": \"just kidding\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"ld\": \"later, dude\", \n",
    "    \"ldr\": \"long distance relationship\", \n",
    "    \"llta\": \"lots and lots of thunderous applause\",\n",
    "    \"lmao\": \"laugh my ass off\", \n",
    "    \"lmirl\": \"let's meet in real life\", \n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"ltr\": \"longterm relationship\",\n",
    "    \"lulab\": \"love you like a brother\",\n",
    "    \"lulas\": \"love you like a sister\", \n",
    "    \"luv\": \"love\", \n",
    "    \"m/f\": \"male or female\", \n",
    "    \"m8\": \"mate\", \n",
    "    \"milf\": \"mother I would like to fuck\", \n",
    "    \"oll\": \"online love\", \n",
    "    \"omg\": \"oh my god\", \n",
    "    \"otoh\": \"on the other hand\", \n",
    "    \"pir\": \"parent in room\", \n",
    "    \"ppl\": \"people\", \n",
    "    \"r\": \"are\",\n",
    "    \"rofl\": \"roll on the floor laughing\", \n",
    "    \"rpg\": \"role playing games\", \n",
    "    \"ru\": \"are you\", \n",
    "    \"shid\": \"slaps head in disgust\",\n",
    "    \"somy\": \"sick of me yet\",\n",
    "    \"sot\": \"short of time\", \n",
    "    \"thanx\": \"thanks\", \n",
    "    \"thx\": \"thanks\", \n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"u\": \"you\", \n",
    "    \"ur\": \"you are\",\n",
    "    \"uw\": \"youâ€™re welcome\", \n",
    "    \"wb\": \"welcome back\", \n",
    "    \"wfm\": \"works for me\", \n",
    "    \"wibni\": \"wouldn't it be nice if\", \n",
    "    \"wtf\": \"what the fuck\", \n",
    "    \"wtg\": \"way to go\",\n",
    "    \"wtgp\": \"want to go private\", \n",
    "    \"ym\": \"young man\", \n",
    "    \"gr8\": \"great\" \n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_tweet'] = test_df['clean_tweet'].apply(lambda x: lookup_dict(x,short_word_dict)) \n",
    "test_df.head(10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" F  Emoticon Lookup\"\n",
    "emoticon_dict = { \":)\": \"happy\", \n",
    "                 \":â€‘)\": \"happy\", \n",
    "                 \":-]\": \"happy\", \n",
    "                 \":-3\": \"happy\", \n",
    "                 \":->\": \"happy\", \n",
    "                 \"8-)\": \"happy\", \n",
    "                 \":-}\": \"happy\", \n",
    "                 \":o)\": \"happy\", \n",
    "                 \":c)\": \"happy\", \n",
    "                 \":^)\": \"happy\", \n",
    "                 \"=]\": \"happy\", \n",
    "                 \"=)\": \"happy\", \n",
    "                 \"<3\": \"happy\", \n",
    "                 \":-(\": \"sad\", \n",
    "                 \":(\": \"sad\", \n",
    "                 \":c\": \"sad\", \n",
    "                 \":<\": \"sad\",\n",
    "                 \":[\": \"sad\", \n",
    "                 \">:[\": \"sad\", \n",
    "                 \":{\": \"sad\", \n",
    "                 \">:(\": \"sad\",\n",
    "                 \":-c\": \"sad\", \n",
    "                 \":-< \": \"sad\",\n",
    "                 \":-[\": \"sad\", \n",
    "                 \":-||\": \"sad\" \n",
    "                } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_tweet'] = test_df['clean_tweet'].apply(lambda x: lookup_dict(x,emoticon_dict)) \n",
    "test_df.head(10) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"G ReplacingPunctuations with space\"\n",
    "test_df['clean_tweet'] = test_df['clean_tweet'].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x)) \n",
    "test_df.head(10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s = \"stringeeee322#@!! With. Punctuation?\"\n",
    "s = re.sub(r'[^\\w\\s]','',s)\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Replacing Special Characters with space\"\n",
    "test_df['clean_tweet'] = test_df['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\n",
    "test_df.head(10) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"J  Removing words whom length is 1\"\n",
    "test_df['clean_tweet'] = test_df['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
    "test_df['clean_tweet'][0:5] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"K  Spelling Correction - With TextBlob Library\"\n",
    "from textblob import TextBlob\n",
    "text = test_df['clean_tweet'][0:10].apply(lambda x: str(TextBlob(x).correct())) \n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tweet_token'] = test_df['clean_tweet'].apply(lambda x: word_tokenize(x))\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words ={\n",
    "    'a',\n",
    "    'about',\n",
    "    'above', \n",
    "    'after',\n",
    "    'again',\n",
    "    'against',\n",
    "    'ain',\n",
    "    'all',\n",
    "    'am',\n",
    "    'an',\n",
    "    'and',\n",
    "    'any', \n",
    "    'are',\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'as',\n",
    "    'at',\n",
    "    'be',\n",
    "    'because',\n",
    "    'been',\n",
    "    'before', \n",
    "    'being',\n",
    "    'below',  \n",
    "    'between',\n",
    "    'both',\n",
    "    'but',\n",
    "    'by', \n",
    "    'can', \n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'd',\n",
    "    'did',\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'do',\n",
    "    'does',  \n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'doing',\n",
    "    'don',\n",
    "    \"don't\", \n",
    "    'down',\n",
    "    'during',  \n",
    "    'each', \n",
    "    'few',\n",
    "    'for',\n",
    "    'from', \n",
    "    'further',\n",
    "    'had',\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'has',\n",
    "    'hasn',  \n",
    "    \"hasn't\",\n",
    "    'have',\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'having',\n",
    "    'he', \n",
    "    'her',  \n",
    "    'here',\n",
    "    'hers',\n",
    "    'herself',\n",
    "    'him', \n",
    "    'himself',\n",
    "    'his', \n",
    "    'how',\n",
    "    'i',\n",
    "    'if',\n",
    "    'in',  \n",
    "    'into',\n",
    "    'is', \n",
    "    'isn',\n",
    "    \"isn't\", \n",
    "    'it', \n",
    "    \"it's\",  \n",
    "    'its',\n",
    "    'itself',\n",
    "    'just',\n",
    "    'll',\n",
    "    'm',\n",
    "    'ma',\n",
    "    'me',\n",
    "    'mightn', \n",
    "    \"mightn't\", \n",
    "    'more',\n",
    "    'most',\n",
    "    'mustn', \n",
    "    \"mustn't\",  \n",
    "    'my', \n",
    "    'myself',\n",
    "    'needn',  \n",
    "    \"needn't\", \n",
    "    'no',\n",
    "    'nor', \n",
    "    'not',  \n",
    "    'now',\n",
    "    'o',\n",
    "    'of',\n",
    "    'off', \n",
    "    'on',  \n",
    "    'once',\n",
    "    'only',\n",
    "    'or',\n",
    "    'other',\n",
    "    'our', \n",
    "    'ours',\n",
    "    'ourselves',\n",
    "    'out',\n",
    "    'over',  \n",
    "    'own', \n",
    "    're',\n",
    "    's',\n",
    "    'same',\n",
    "    'shan',  \n",
    "    \"shan't\",\n",
    "    'she', \n",
    "    \"she's\",\n",
    "    'should', \n",
    "    \"should've\",\n",
    "    'shouldn', \n",
    "    \"shouldn't\",\n",
    "    'so', \n",
    "    'some',\n",
    "    'such',\n",
    "     't', \n",
    "    'than', \n",
    "    'that',\n",
    "    \"that'll\",\n",
    "    'the', \n",
    "    'their',  \n",
    "    'theirs', \n",
    "    'them',  \n",
    "    'themselves',\n",
    "    'then',\n",
    "    'there',  \n",
    "    'these', \n",
    "    'they', \n",
    "    'this',\n",
    "    'those',  \n",
    "    'through',\n",
    "    'to',\n",
    "    'too', \n",
    "    'under', \n",
    "    'until',\n",
    "    'up',\n",
    "    've',\n",
    "    'very',  \n",
    "    'was',\n",
    "    'wasn', \n",
    "    \"wasn't\",\n",
    "    'we',\n",
    "    'were',  \n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    'what', \n",
    "    'when', \n",
    "    'where',  \n",
    "    'which',\n",
    "    'while',  \n",
    "    'who',\n",
    "    'whom',\n",
    "    'why',\n",
    "    'will',\n",
    "    'with', \n",
    "    'won', \n",
    "    \"won't\",\n",
    "     't', \n",
    "    'than',\n",
    "    'that',\n",
    "    \"that'll\",\n",
    "    'the', \n",
    "    'their',  \n",
    "    'theirs', \n",
    "    'them',\n",
    "    'themselves',\n",
    "    'then',  \n",
    "    'there', \n",
    "    'these',  \n",
    "    'they',\n",
    "    'this',  \n",
    "    'those',\n",
    "    'through',\n",
    "    'to',\n",
    "    'too', \n",
    "    'under',\n",
    "    'until',  \n",
    "    'up',  \n",
    "    've',  \n",
    "    'very',  \n",
    "    'was',  \n",
    "    'wasn',\n",
    "    \"wasn't\",\n",
    "    'we', \n",
    "    'were',\n",
    "    'weren', \n",
    "    \"weren't\", \n",
    "    'what', \n",
    "    'when',  \n",
    "    'where', \n",
    "    'which',\n",
    "    'while', \n",
    "    'who', \n",
    "    'whom',  \n",
    "    'why',\n",
    "    'will', \n",
    "    'with',\n",
    "    'won',\n",
    "    \"won't\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tweet_token_filtered'] = test_df['tweet_token'].apply(lambda x: [word for word in x if not word in stop_words]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df[['tweet_token', 'tweet_token_filtered']].head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tweet_stemmed'] = test_df['tweet_token_filtered'].apply(lambda x: ' '.join([stemming.stem(i) for i in x])) \n",
    "test_df['tweet_stemmed'].head(10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lemmatizing = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df['tweet_lemmatized'] = test_df['tweet_token_filtered'].apply(lambda x:' '.join([lemmatizing.lemmatize(i) for i in x])) \n",
    "test_df['tweet_lemmatized'].head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "cv= CountVectorizer() \n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=cv.fit(test_df[\"tweet_lemmatized\"])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cv.transform(test_df[\"tweet_lemmatized\"])\n",
    "y=test_df[\"Sentiment\"]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trian,x_test,y_train,y_test = train_test_split(x,y,train_size=0.80)\n",
    "print(x_trian.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256,activation=\"relu\"))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(128,activation=\"relu\"))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(64,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(2,activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_trian,y_trian,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"for c in [0.1,0.05,0.25,0.32,0.5,3]:\n",
    "    lr=LogisticRegression(C=c)\n",
    "    lr.fit(x_trian,y_train)\n",
    "    print('accuracy for c=%s:%s'%(c,accuracy_score(y_test,lr.predict(x_test))))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"for c in [0.1,0.05,0.25,0.32,0.5,0.0000002]:\n",
    "    sv=SVC(C=c,kernel='poly',degree=3,gamma=10)\n",
    "    sv.fit(x_trian,y_train)\n",
    "    print('accuracy for c=%s:%s'%(c,accuracy_score(y_test,lr.predict(x_test))))\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
